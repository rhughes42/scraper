{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d9a5f8",
   "metadata": {},
   "source": [
    "# üèõÔ∏è CURIA Scraper - Complete Web Scraping Pipeline\n",
    "\n",
    "This notebook provides a comprehensive solution for scraping legal documents from the **CURIA (Court of Justice of the European Union)** website. It automates the entire process from navigating search results to downloading PDFs and extracting structured metadata.\n",
    "\n",
    "## ‚ú® Key Features\n",
    "\n",
    "- **üîç Smart Document Discovery**: Automatically finds and processes CURIA document links\n",
    "- **üìÑ PDF Generation**: Clicks \"Start Printing\" buttons to generate clean PDF downloads  \n",
    "- **üåê Multi-language Support**: Filters documents by preferred language (EN, FR, DE, etc.)\n",
    "- **üìä Structured Data**: Extracts case numbers, titles, dates, and metadata\n",
    "- **üîÑ Pagination Handling**: Automatically processes multiple result pages\n",
    "- **‚ö° Error Recovery**: Graceful fallbacks for failed downloads\n",
    "\n",
    "## üöÄ Quick Start Guide\n",
    "\n",
    "### 1. **Prerequisites**\n",
    "- Python 3.8+ with Jupyter support\n",
    "- Internet connection for CURIA website access\n",
    "- Sufficient disk space for PDF downloads\n",
    "\n",
    "### 2. **Configuration** \n",
    "The scraper auto-generates a `config.toml` file with CURIA-optimized defaults:\n",
    "- **Target Language**: English (EN) - modify `preferred_language` as needed\n",
    "- **Search URL**: CURIA search results page - update `listing_url` with your specific search\n",
    "- **Output Directory**: `./output` - all files saved here\n",
    "- **Browser Mode**: Visible by default (`headless = false`) for debugging\n",
    "\n",
    "### 3. **What It Does**\n",
    "1. ‚úÖ Navigates through CURIA search result pages\n",
    "2. ‚úÖ Identifies document links matching your language preference  \n",
    "3. ‚úÖ Clicks \"Start Printing\" buttons when available for clean PDFs\n",
    "4. ‚úÖ Falls back to HTML parsing if print buttons unavailable\n",
    "5. ‚úÖ Extracts structured metadata (case numbers, titles, dates)\n",
    "6. ‚úÖ Handles pagination automatically across multiple pages\n",
    "7. ‚úÖ Saves everything with meaningful filenames (`curia-doc-{id}.pdf`)\n",
    "\n",
    "### 4. **Example CURIA URLs**\n",
    "- **Search Results**: `https://curia.europa.eu/juris/recherche.jsf?language=en`\n",
    "- **Document Page**: `https://curia.europa.eu/juris/document/document.jsf?docid=304744&doclang=EN`\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "- **Windows Users**: Due to subprocess limitations, the scraper includes a standalone Python script fallback\n",
    "- **Rate Limiting**: Built-in delays prevent overwhelming the CURIA servers\n",
    "- **Legal Compliance**: Ensure your usage complies with CURIA's terms of service\n",
    "- **Testing**: Set `headless = false` in config.toml to watch the browser during debugging\n",
    "\n",
    "## üìÅ Output Structure\n",
    "\n",
    "```\n",
    "output/\n",
    "‚îú‚îÄ‚îÄ curia-doc-12345.pdf      # PDF documents (when print button works)\n",
    "‚îú‚îÄ‚îÄ doc_1.json               # Structured metadata for each document\n",
    "‚îú‚îÄ‚îÄ doc_2.json               # Contains URLs, case numbers, titles, etc.\n",
    "‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "Ready to start? Run the cells below in order! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a939304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is required on first run to install Playwright browsers so they work async etc.\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0caaac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Package Status: 7 of 7 packages already installed\n",
      "‚úÖ All required packages already installed\n",
      "\n",
      "üé≠ Installing Playwright browsers...\n",
      "\n",
      "üéâ Installation complete! All dependencies are ready.\n",
      "üí° Tip: If you encounter Windows/Jupyter issues, use the standalone script generated at the end.\n",
      "\n",
      "üéâ Installation complete! All dependencies are ready.\n",
      "üí° Tip: If you encounter Windows/Jupyter issues, use the standalone script generated at the end.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Install Required Python Packages\n",
    "# This cell automatically detects and installs missing dependencies\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from playwright.sync_api import sync_playwright\n",
    "import importlib.util\n",
    "\n",
    "def check_package(package_name):\n",
    "    \"\"\"Check if a package is installed\"\"\"\n",
    "    spec = importlib.util.find_spec(package_name)\n",
    "    return spec is not None\n",
    "\n",
    "def check_playwright_browsers():\n",
    "    \"\"\"Check if playwright browsers are installed\"\"\"\n",
    "    try:\n",
    "        with sync_playwright() as p:\n",
    "            # Try to get browser executable path - if it fails, browsers aren't installed\n",
    "            p.chromium.executable_path\n",
    "            return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Define packages to check (added nest-asyncio for Windows/Jupyter compatibility)\n",
    "packages_to_check = [\n",
    "    (\"playwright\", \"playwright\"),\n",
    "    (\"bs4\", \"beautifulsoup4\"),\n",
    "    (\"pydantic\", \"pydantic\"),\n",
    "    (\"toml\", \"toml\"),\n",
    "    (\"nest_asyncio\", \"nest-asyncio\"),\n",
    "    (\"requests\", \"requests\"),\n",
    "    (\"urllib3\", \"urllib3\")\n",
    "]\n",
    "\n",
    "installed_packages = []\n",
    "missing_packages = []\n",
    "\n",
    "# Check each package\n",
    "for import_name, pip_name in packages_to_check:\n",
    "    if check_package(import_name):\n",
    "        installed_packages.append(pip_name)\n",
    "    else:\n",
    "        missing_packages.append(pip_name)\n",
    "\n",
    "# Check playwright browsers separately\n",
    "playwright_browsers_installed = check_playwright_browsers() if check_package(\"playwright\") else False\n",
    "\n",
    "print(f\"üì¶ Package Status: {len(installed_packages)} of {len(packages_to_check)} packages already installed\")\n",
    "\n",
    "# Install missing packages\n",
    "if missing_packages:\n",
    "    print(f\"\\nüîß Installing missing packages: {', '.join(missing_packages)}\")\n",
    "    for package in missing_packages:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], check=True)\n",
    "else:\n",
    "    print(\"‚úÖ All required packages already installed\")\n",
    "\n",
    "# Install playwright browsers if needed\n",
    "if check_package(\"playwright\") and not playwright_browsers_installed:\n",
    "    print(\"\\nüé≠ Installing Playwright browsers...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"playwright\", \"install\"], check=True)\n",
    "elif not check_package(\"playwright\"):\n",
    "    print(\"\\nüé≠ Installing Playwright browsers...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"playwright\", \"install\"], check=True)\n",
    "else:\n",
    "    print(\"‚úÖ Playwright browsers already installed\")\n",
    "\n",
    "print(\"\\nüéâ Installation complete! All dependencies are ready.\")\n",
    "print(\"üí° Tip: If you encounter Windows/Jupyter issues, use the standalone script generated at the end.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967bd61",
   "metadata": {},
   "source": [
    "# Step 3: Import Core Libraries\n",
    "Import all required libraries for web scraping, data processing, and file management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea406946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core library imports for the CURIA scraper\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import toml\n",
    "from pydantic import BaseModel\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, Page\n",
    "import logging\n",
    "\n",
    "print(\"‚úÖ Core libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edddc364",
   "metadata": {},
   "source": [
    "# Step 4: Configuration Setup\n",
    "Load or create the configuration file with CURIA-specific settings and validation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "440aafff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded and validated successfully!\n",
      "\n",
      "üìã CURIA Scraper Configuration:\n",
      "   üåê Target Language: en\n",
      "   üîó Listing URL: https://curia.europa.eu/juris/documents.jsf?nat=or&mat=or&pcs=Oor&jur=C%2CT%2CF&for=&jge=&dates=%2524type%253Dpro%2524mode%253D1M%2524from%253D2025.09.27%2524to%253D2025.10.27&language=en&pro=&cit=none%252CC%252CCJ%252CR%252C2008E%252C%252C%252C%252C%252C%252C%252C%252C%252C%252Ctrue%252Cfalse%252Cfalse&oqp=&td=%24mode%3D1M%24from%3D2025.09.27%24to%3D2025.10.27%3B%3BPUB%3BPUB1%2CPUB2%2CPUB4%2CPUB7%2CPUB3%2CPUB8%2CPUB5%2CPUB6%3B%3B%3B%3BORDALL&avg=&lgrec=en&page=1&lg=EN%252C%252Btrue%252Cfalse&cid=7416927\n",
      "   üëÅÔ∏è  Headless Mode: True\n",
      "   üìÅ Output Directory: ./output\n",
      "   üéØ Document Selector: div#docHtml a[href*='document.jsf']\n",
      "   ‚ñ∂Ô∏è  Next Page Selector: a[title='Next Page']\n",
      "\n",
      "üí° Edit config.toml to customize settings before running the scraper!\n"
     ]
    }
   ],
   "source": [
    "# Configuration models using Pydantic for validation\n",
    "class GeneralSettings(BaseModel):\n",
    "    output_dir: str\n",
    "    checkpoint_file: str\n",
    "    headless: bool\n",
    "    throttle_delay_ms: int\n",
    "    preferred_language: str = \"EN\"  # Language code for CURIA documents\n",
    "\n",
    "class SiteSettings(BaseModel):\n",
    "    listing_url: str\n",
    "    document_content_selector: str = \"body\"\n",
    "    start_print_button_text: str = \"Start Printing\"\n",
    "    document_link_selector: str = \"div#docHtml a[href*='document.jsf']\"\n",
    "    next_page_selector: str = \"a[title='Next Page']\"\n",
    "\n",
    "class Settings(BaseModel):\n",
    "    general: GeneralSettings\n",
    "    site: SiteSettings\n",
    "\n",
    "# Create or load configuration\n",
    "config_path = Path(\"config.toml\")\n",
    "\n",
    "if not config_path.exists():\n",
    "    print(\"üìù Creating default configuration file...\")\n",
    "    default_config = {\n",
    "        \"general\": {\n",
    "            \"output_dir\": \"./output\",\n",
    "            \"checkpoint_file\": \"./checkpoint.json\",\n",
    "            \"headless\": False,  # Visible browser for debugging\n",
    "            \"throttle_delay_ms\": 2000,\n",
    "            \"preferred_language\": \"EN\"\n",
    "        },\n",
    "        \"site\": {\n",
    "            \"listing_url\": \"https://curia.europa.eu/juris/recherche.jsf?language=en\",\n",
    "            \"document_content_selector\": \"body\",\n",
    "            \"start_print_button_text\": \"Start Printing\",\n",
    "            \"document_link_selector\": \"div#docHtml a[href*='document.jsf']\",\n",
    "            \"next_page_selector\": \"a[title='Next Page']\"\n",
    "        }\n",
    "    }\n",
    "    with open(config_path, \"w\") as f:\n",
    "        toml.dump(default_config, f)\n",
    "    print(\"‚úÖ Created config.toml with CURIA defaults\")\n",
    "\n",
    "# Load and validate configuration\n",
    "try:\n",
    "    config_data = toml.load(\"config.toml\")\n",
    "\n",
    "    # Ensure all required fields exist with backward compatibility\n",
    "    if \"site\" in config_data:\n",
    "        site_config = config_data[\"site\"]\n",
    "\n",
    "        # Add missing fields with defaults\n",
    "        defaults = {\n",
    "            \"document_link_selector\": \"div#docHtml a[href*='document.jsf']\",\n",
    "            \"next_page_selector\": \"a[title='Next Page']\",\n",
    "            \"document_content_selector\": \"body\",\n",
    "            \"start_print_button_text\": \"Start Printing\"\n",
    "        }\n",
    "\n",
    "        for key, default_value in defaults.items():\n",
    "            if key not in site_config:\n",
    "                site_config[key] = default_value\n",
    "                print(f\"‚ûï Added missing config field: {key}\")\n",
    "\n",
    "    if \"general\" in config_data:\n",
    "        general_config = config_data[\"general\"]\n",
    "        if \"preferred_language\" not in general_config:\n",
    "            general_config[\"preferred_language\"] = \"EN\"\n",
    "            print(\"‚ûï Added missing preferred_language field\")\n",
    "\n",
    "    # Save updated configuration\n",
    "    with open(config_path, \"w\") as f:\n",
    "        toml.dump(config_data, f)\n",
    "\n",
    "    settings = Settings(**config_data)\n",
    "    print(\"‚úÖ Configuration loaded and validated successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Configuration error: {e}\")\n",
    "    print(\"üîÑ Creating fresh configuration...\")\n",
    "\n",
    "    # Fallback to fresh config\n",
    "    default_config = {\n",
    "        \"general\": {\n",
    "            \"output_dir\": \"./output\",\n",
    "            \"checkpoint_file\": \"./checkpoint.json\",\n",
    "            \"headless\": False,\n",
    "            \"throttle_delay_ms\": 2000,\n",
    "            \"preferred_language\": \"EN\"\n",
    "        },\n",
    "        \"site\": {\n",
    "            \"listing_url\": \"https://curia.europa.eu/juris/recherche.jsf?language=en\",\n",
    "            \"document_content_selector\": \"body\",\n",
    "            \"start_print_button_text\": \"Start Printing\",\n",
    "            \"document_link_selector\": \"div#docHtml a[href*='document.jsf']\",\n",
    "            \"next_page_selector\": \"a[title='Next Page']\"\n",
    "        }\n",
    "    }\n",
    "    with open(config_path, \"w\") as f:\n",
    "        toml.dump(default_config, f)\n",
    "    settings = Settings(**default_config)\n",
    "    print(\"‚úÖ Fresh configuration created!\")\n",
    "\n",
    "# Display current configuration\n",
    "print(f\"\\nüìã CURIA Scraper Configuration:\")\n",
    "print(f\"   üåê Target Language: {settings.general.preferred_language}\")\n",
    "print(f\"   üîó Listing URL: {settings.site.listing_url}\")\n",
    "print(f\"   üëÅÔ∏è  Headless Mode: {settings.general.headless}\")\n",
    "print(f\"   üìÅ Output Directory: {settings.general.output_dir}\")\n",
    "print(f\"   üéØ Document Selector: {settings.site.document_link_selector}\")\n",
    "print(f\"   ‚ñ∂Ô∏è  Next Page Selector: {settings.site.next_page_selector}\")\n",
    "print(f\"\\nüí° Edit config.toml to customize settings before running the scraper!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d946f2",
   "metadata": {},
   "source": [
    "# Step 5: Logging Configuration\n",
    "Set up structured logging to track scraper progress and debug issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92ba105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logging configured - scraper progress will be displayed with timestamps\n"
     ]
    }
   ],
   "source": [
    "# Configure logging for the CURIA scraper\n",
    "logger = logging.getLogger(\"curia_scraper\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "print(\"‚úÖ Logging configured - scraper progress will be displayed with timestamps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ff3d6",
   "metadata": {},
   "source": [
    "# Step 6: Browser Management\n",
    "Define the browser manager class for handling Playwright browser instances and downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de59b622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Browser manager class defined - ready for web automation\n"
     ]
    }
   ],
   "source": [
    "# Browser manager for Playwright automation\n",
    "class BrowserManager:\n",
    "    \"\"\"Manages Playwright browser instances with proper cleanup\"\"\"\n",
    "\n",
    "    def __init__(self, headless: bool = True, downloads_path: str = None):\n",
    "        self.headless = headless\n",
    "        self.downloads_path = downloads_path\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        \"\"\"Start Playwright and launch browser\"\"\"\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(headless=self.headless)\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        \"\"\"Clean up browser and Playwright resources\"\"\"\n",
    "        if self.browser:\n",
    "            await self.browser.close()\n",
    "        await self.playwright.stop()\n",
    "\n",
    "    async def new_page(self):\n",
    "        \"\"\"Create a new browser page with download configuration\"\"\"\n",
    "        context_args = {}\n",
    "        if self.downloads_path:\n",
    "            context_args[\"accept_downloads\"] = True\n",
    "            context_args[\"downloads_path\"] = self.downloads_path\n",
    "        context = await self.browser.new_context(**context_args)\n",
    "        page = await context.new_page()\n",
    "        return page\n",
    "\n",
    "print(\"‚úÖ Browser manager class defined - ready for web automation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82bf9c",
   "metadata": {},
   "source": [
    "# Step 7: HTML Parser Functions\n",
    "CURIA-specific functions for extracting structured data from legal documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a84d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CURIA document parser functions defined\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for parsing\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def parse_curia_document(html: str, url: str, doc_id: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Parse CURIA legal document HTML and extract structured metadata\n",
    "\n",
    "    Args:\n",
    "        html: Raw HTML content of the document\n",
    "        url: Document URL for context\n",
    "        doc_id: Optional document ID for reference\n",
    "\n",
    "    Returns:\n",
    "        dict: Structured document metadata\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Extract case number using multiple approaches\n",
    "    case_number = None\n",
    "    case_selectors = [\n",
    "        \"span.case-number\",\n",
    "        \"div.case-number\",\n",
    "        \"h1\", \"h2\", \"h3\"  # Often in headings\n",
    "    ]\n",
    "\n",
    "    # Try CSS selectors first\n",
    "    for selector in case_selectors:\n",
    "        tag = soup.select_one(selector)\n",
    "        if tag and tag.text.strip():\n",
    "            text = tag.text.strip()\n",
    "            # Look for case pattern in the text\n",
    "            case_match = re.search(r'Case\\s+[A-Z]-\\d+/\\d+', text, re.I)\n",
    "            if case_match:\n",
    "                case_number = case_match.group(0)\n",
    "                break\n",
    "\n",
    "    # Fallback: search all text for case patterns\n",
    "    if not case_number:\n",
    "        text_content = soup.get_text()\n",
    "        case_matches = re.findall(r'Case\\s+[A-Z]-\\d+/\\d+', text_content, re.I)\n",
    "        if case_matches:\n",
    "            case_number = case_matches[0]\n",
    "\n",
    "    # Extract document title\n",
    "    title = None\n",
    "    title_selectors = [\"title\", \"h1\", \"h2\", \".document-title\", \".judgment-title\"]\n",
    "    for selector in title_selectors:\n",
    "        tag = soup.select_one(selector)\n",
    "        if tag and tag.text.strip():\n",
    "            title = tag.text.strip()\n",
    "            # Clean up common title prefixes\n",
    "            title = re.sub(r'^(CURIA\\s*-\\s*)?', '', title, flags=re.I)\n",
    "            break\n",
    "\n",
    "    # Extract date of judgment using various patterns\n",
    "    date_of_judgment = None\n",
    "    date_patterns = [\n",
    "        r'\\b\\d{1,2}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{4}\\b',        # DD/MM/YYYY\n",
    "        r'\\b\\d{4}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{1,2}\\b',        # YYYY/MM/DD\n",
    "        r'\\b\\d{1,2}\\s+\\w+\\s+\\d{4}\\b'                       # DD Month YYYY\n",
    "    ]\n",
    "\n",
    "    text_content = soup.get_text()\n",
    "    for pattern in date_patterns:\n",
    "        matches = re.findall(pattern, text_content)\n",
    "        if matches:\n",
    "            date_of_judgment = matches[0]\n",
    "            break\n",
    "\n",
    "    # Extract parties from structured tables\n",
    "    parties = []\n",
    "    tables = soup.find_all(\"table\")\n",
    "    for table in tables:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cells = row.find_all([\"td\", \"th\"])\n",
    "            if len(cells) >= 2:\n",
    "                label = cells[0].get_text(strip=True).lower()\n",
    "                value = cells[1].get_text(strip=True)\n",
    "                # Look for party-related information\n",
    "                if any(keyword in label for keyword in [\"party\", \"applicant\", \"defendant\", \"member state\"]):\n",
    "                    if value and value not in parties:\n",
    "                        parties.append(value)\n",
    "\n",
    "    # Extract language from URL\n",
    "    language = None\n",
    "    lang_match = re.search(r'doclang=([A-Z]{2})', url)\n",
    "    if lang_match:\n",
    "        language = lang_match.group(1)\n",
    "\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"url\": url,\n",
    "        \"language\": language,\n",
    "        \"case_number\": case_number,\n",
    "        \"title\": title,\n",
    "        \"date_of_judgment\": date_of_judgment,\n",
    "        \"parties\": parties,\n",
    "        \"html_length\": len(html),\n",
    "        \"extracted_at\": str(datetime.now()),\n",
    "        \"html\": html  # Include full HTML for reference\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ CURIA document parser functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6ee1d",
   "metadata": {},
   "source": [
    "# Step 8: File Storage Functions\n",
    "Functions for saving scraped data in JSON format with proper organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5113853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Storage functions defined - ready to save scraped data\n"
     ]
    }
   ],
   "source": [
    "# Storage functions for saving scraped data\n",
    "def save_json(content: dict, idx: int):\n",
    "    \"\"\"\n",
    "    Save document metadata or content to a JSON file\n",
    "\n",
    "    Args:\n",
    "        content: Dictionary containing document data\n",
    "        idx: Index number for filename\n",
    "    \"\"\"\n",
    "    output_dir = Path(settings.general.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filename = output_dir / f\"doc_{idx}.json\"\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(content, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    logger.info(f\"Saved JSON metadata: {filename}\")\n",
    "\n",
    "print(\"‚úÖ Storage functions defined - ready to save scraped data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5f8ee",
   "metadata": {},
   "source": [
    "# Step 9: Document Processing Pipeline\n",
    "Core function for processing individual CURIA documents with PDF generation and metadata extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2db0efc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document processing function defined\n"
     ]
    }
   ],
   "source": [
    "# Document processor with CURIA-specific workflow\n",
    "async def process_document(page: Page, doc_link: str, idx: int):\n",
    "    \"\"\"\n",
    "    Process a single CURIA document: navigate, find print button, generate PDF or save HTML\n",
    "\n",
    "    Args:\n",
    "        page: Playwright page instance\n",
    "        doc_link: URL of the document to process\n",
    "        idx: Document index for naming\n",
    "    \"\"\"\n",
    "    logger.info(f\"[{idx}] Processing CURIA document: {doc_link}\")\n",
    "\n",
    "    # Navigate to the document page\n",
    "    await page.goto(doc_link)\n",
    "    await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "    # Extract document ID from URL for better filename\n",
    "    doc_id_match = re.search(r'docid=(\\d+)', doc_link)\n",
    "    doc_id = doc_id_match.group(1) if doc_id_match else str(idx)\n",
    "\n",
    "    log_entry = {\n",
    "        \"idx\": idx,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"url\": doc_link,\n",
    "        \"used_path\": None,\n",
    "        \"filename\": None\n",
    "    }\n",
    "\n",
    "    # Look for the \"Start Printing\" button (CURIA specific)\n",
    "    print_button_selectors = [\n",
    "        \"input[value*='Start Printing']\",\n",
    "        \"button:has-text('Start Printing')\",\n",
    "        \"a:has-text('Start Printing')\",\n",
    "        \"input[type='submit'][value*='Print']\",\n",
    "        \"*:has-text('Start Printing')\"\n",
    "    ]\n",
    "\n",
    "    print_btn = None\n",
    "    for selector in print_button_selectors:\n",
    "        try:\n",
    "            print_btn = await page.wait_for_selector(selector, timeout=3000)\n",
    "            if print_btn:\n",
    "                logger.info(f\"[{idx}] Found 'Start Printing' button with selector: {selector}\")\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if print_btn:\n",
    "        # Click the print button and wait for the print-ready page\n",
    "        await print_btn.click()\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "        # Wait for print content to load properly\n",
    "        await page.wait_for_timeout(2000)\n",
    "\n",
    "        filename = f\"curia-doc-{doc_id}.pdf\"\n",
    "        filepath = Path(settings.general.output_dir) / filename\n",
    "\n",
    "        try:\n",
    "            # Set print media emulation for cleaner PDFs\n",
    "            await page.emulate_media(media=\"print\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[{idx}] Media emulation failed: {e}\")\n",
    "\n",
    "        # Generate PDF with proper formatting\n",
    "        await page.pdf(\n",
    "            path=str(filepath),\n",
    "            format=\"A4\",\n",
    "            print_background=True,\n",
    "            margin={\n",
    "                \"top\": \"1cm\",\n",
    "                \"right\": \"1cm\",\n",
    "                \"bottom\": \"1cm\",\n",
    "                \"left\": \"1cm\"\n",
    "            }\n",
    "        )\n",
    "        logger.info(f\"[{idx}] Saved PDF: {filepath}\")\n",
    "\n",
    "        log_entry[\"used_path\"] = \"page_pdf\"\n",
    "        log_entry[\"filename\"] = filename\n",
    "        save_json(log_entry, idx)\n",
    "        return\n",
    "\n",
    "    # Fallback: if no print button found, save HTML content with metadata\n",
    "    logger.info(f\"[{idx}] No 'Start Printing' button found; extracting HTML content\")\n",
    "\n",
    "    try:\n",
    "        # Get the main content area\n",
    "        content_element = await page.query_selector(\"body\")\n",
    "        html_content = await content_element.inner_html() if content_element else await page.content()\n",
    "\n",
    "        # Parse the HTML and extract structured data\n",
    "        parsed = parse_curia_document(html_content, doc_link, doc_id)\n",
    "        parsed[\"filename\"] = None\n",
    "        parsed[\"used_path\"] = \"html_parse\"\n",
    "        save_json(parsed, idx)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[{idx}] Error processing document: {e}\")\n",
    "        log_entry[\"error\"] = str(e)\n",
    "        save_json(log_entry, idx)\n",
    "\n",
    "print(\"‚úÖ Document processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba5d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Main listing processor function defined\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Main Listing Processor\n",
    "# Core scraper function that handles pagination, document discovery, and batch processing\n",
    "\n",
    "async def process_listing():\n",
    "    \"\"\"\n",
    "    Main CURIA listing processor: navigate search results, find documents, handle pagination\n",
    "    \"\"\"\n",
    "    output_dir = Path(settings.general.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    async with BrowserManager(headless=settings.general.headless, downloads_path=str(output_dir)) as bm:\n",
    "        page = await bm.new_page()\n",
    "\n",
    "        logger.info(f\"Navigating to: {settings.site.listing_url}\")\n",
    "        await page.goto(settings.site.listing_url)\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "        idx = 0\n",
    "        page_num = 1\n",
    "\n",
    "        while True:\n",
    "            logger.info(f\"Processing page {page_num}...\")\n",
    "\n",
    "            # Wait for page content to load\n",
    "            try:\n",
    "                await page.wait_for_selector(\"table, .results, .judgment-list\", timeout=10000)\n",
    "            except:\n",
    "                logger.warning(\"No standard content selectors found, continuing anyway...\")\n",
    "\n",
    "            # Find document links using multiple selectors for robustness\n",
    "            document_links = []\n",
    "            link_selectors = [\n",
    "                settings.site.document_link_selector,          # From config\n",
    "                \"div#docHtml a[href*='document.jsf']\",         # CURIA specific\n",
    "                \"a[href*='document.jsf']\",                     # More general\n",
    "                \"a[href*='docid=']\"                            # Even more general\n",
    "            ]\n",
    "\n",
    "            for selector in link_selectors:\n",
    "                try:\n",
    "                    links = await page.query_selector_all(selector)\n",
    "                    if links:\n",
    "                        logger.info(f\"Found {len(links)} document links using selector: {selector}\")\n",
    "\n",
    "                        for link in links:\n",
    "                            href = await link.get_attribute(\"href\")\n",
    "                            if href:\n",
    "                                # Ensure absolute URL\n",
    "                                if href.startswith(\"http\"):\n",
    "                                    full_url = href\n",
    "                                else:\n",
    "                                    base_url = \"https://curia.europa.eu\"\n",
    "                                    full_url = f\"{base_url}{href}\" if href.startswith(\"/\") else f\"{base_url}/{href}\"\n",
    "\n",
    "                                # Filter for preferred language if specified\n",
    "                                if settings.general.preferred_language:\n",
    "                                    if f\"doclang={settings.general.preferred_language}\" in full_url:\n",
    "                                        document_links.append(full_url)\n",
    "                                    elif \"doclang=\" not in full_url:\n",
    "                                        # Add language parameter if not present\n",
    "                                        lang_param = f\"&doclang={settings.general.preferred_language}\"\n",
    "                                        full_url += lang_param\n",
    "                                        document_links.append(full_url)\n",
    "                                else:\n",
    "                                    document_links.append(full_url)\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Selector '{selector}' failed: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if not document_links:\n",
    "                logger.warning(\"No document links found on this page\")\n",
    "                break\n",
    "\n",
    "            # Remove duplicates while preserving order\n",
    "            unique_links = []\n",
    "            seen = set()\n",
    "            for link in document_links:\n",
    "                # Extract docid for deduplication\n",
    "                doc_match = re.search(r'docid=(\\d+)', link)\n",
    "                if doc_match:\n",
    "                    doc_id = doc_match.group(1)\n",
    "                    if doc_id not in seen:\n",
    "                        seen.add(doc_id)\n",
    "                        unique_links.append(link)\n",
    "\n",
    "            logger.info(f\"Processing {len(unique_links)} unique documents from page {page_num}\")\n",
    "\n",
    "            # Process each document\n",
    "            for doc_link in unique_links:\n",
    "                idx += 1\n",
    "                logger.info(f\"[{idx}] Processing document: {doc_link}\")\n",
    "\n",
    "                try:\n",
    "                    await process_document(page, doc_link, idx)\n",
    "                    # Small delay between documents to avoid overwhelming the server\n",
    "                    await page.wait_for_timeout(1000)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"[{idx}] Error processing {doc_link}: {e}\")\n",
    "\n",
    "            # Look for next page button\n",
    "            next_btn = None\n",
    "            next_selectors = [\n",
    "                settings.site.next_page_selector,\n",
    "                \"a[title*='Next']\",\n",
    "                \"a[title*='next']\",\n",
    "                \"a:has-text('Next')\",\n",
    "                \"a:has-text('¬ª')\",\n",
    "                \".next-page\",\n",
    "                \"input[value*='Next']\"\n",
    "            ]\n",
    "\n",
    "            for selector in next_selectors:\n",
    "                try:\n",
    "                    next_btn = await page.query_selector(selector)\n",
    "                    if next_btn:\n",
    "                        # Check if button is enabled\n",
    "                        is_disabled = await next_btn.get_attribute(\"disabled\")\n",
    "                        if not is_disabled:\n",
    "                            logger.info(f\"Found next page button with selector: {selector}\")\n",
    "                            break\n",
    "                        else:\n",
    "                            next_btn = None\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if not next_btn:\n",
    "                logger.info(\"No next page button found or all pages processed\")\n",
    "                break\n",
    "\n",
    "            # Navigate to next page\n",
    "            logger.info(f\"Moving to page {page_num + 1}\")\n",
    "            await next_btn.click()\n",
    "            await page.wait_for_load_state(\"networkidle\")\n",
    "            await page.wait_for_timeout(settings.general.throttle_delay_ms)\n",
    "            page_num += 1\n",
    "\n",
    "        logger.info(f\"Scraping completed. Processed {idx} total documents across {page_num} pages.\")\n",
    "\n",
    "print(\"‚úÖ Main listing processor function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e01ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-18' coro=<Connection.run() done, defined at c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\site-packages\\playwright\\_impl\\_connection.py:303> exception=NotImplementedError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 310, in run\n",
      "    await self._transport.connect()\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 133, in connect\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\asyncio\\base_events.py\", line 503, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting CURIA scraper...\n",
      "üìÅ Output directory: ./output\n",
      "üåê Target URL: https://curia.europa.eu/juris/documents.jsf?nat=or&mat=or&pcs=Oor&jur=C%2CT%2CF&for=&jge=&dates=%2524type%253Dpro%2524mode%253D1M%2524from%253D2025.09.27%2524to%253D2025.10.27&language=en&pro=&cit=none%252CC%252CCJ%252CR%252C2008E%252C%252C%252C%252C%252C%252C%252C%252C%252C%252Ctrue%252Cfalse%252Cfalse&oqp=&td=%24mode%3D1M%24from%3D2025.09.27%24to%3D2025.10.27%3B%3BPUB%3BPUB1%2CPUB2%2CPUB4%2CPUB7%2CPUB3%2CPUB8%2CPUB5%2CPUB6%3B%3B%3B%3BORDALL&avg=&lgrec=en&page=1&lg=EN%252C%252Btrue%252Cfalse&cid=7416927\n",
      "üîç Headless mode: True\n",
      "üåç Language filter: en\n",
      "üîÑ Attempting direct async execution...\n",
      "‚ö†Ô∏è Direct execution failed: NotImplementedError\n",
      "üîÑ Switching to threaded execution...\n",
      "\n",
      "üîß Using threaded execution for Windows compatibility...\n",
      "‚úÖ Output directory ready: d:\\Repos\\Active\\Client\\Pandektes\\scraper\\output\n",
      "‚ùå Error during scraping: \n",
      "Error type: NotImplementedError\n",
      "\n",
      "üîç Unexpected error: \n",
      "\n",
      "üìù Full traceback:\n",
      "‚ùå Scraping failed - check errors above\n",
      "\n",
      "üìã Scraper execution complete. Check the output directory for results.\n",
      "üí° If you encountered Windows/subprocess issues, try the standalone script)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ryan\\AppData\\Local\\Temp\\ipykernel_39716\\1283879018.py\", line 32, in run_scraper_sync\n",
      "    result = loop.run_until_complete(process_listing())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ryan\\AppData\\Local\\Temp\\ipykernel_39716\\1213939597.py\", line 11, in process_listing\n",
      "    async with BrowserManager(headless=settings.general.headless, downloads_path=str(output_dir)) as bm:\n",
      "  File \"C:\\Users\\Ryan\\AppData\\Local\\Temp\\ipykernel_39716\\3767045468.py\", line 11, in __aenter__\n",
      "    self.playwright = await async_playwright().start()\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py\", line 51, in start\n",
      "    return await self.__aenter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py\", line 46, in __aenter__\n",
      "    playwright = AsyncPlaywright(next(iter(done)).result())\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ryan\\anaconda3\\envs\\curia-scraper\\Lib\\asyncio\\base_events.py\", line 503, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Execute CURIA Scraper\n",
    "# Main execution cell with Windows/Jupyter compatibility and comprehensive error handling\n",
    "\n",
    "import nest_asyncio\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"üöÄ Starting CURIA scraper...\")\n",
    "print(f\"üìÅ Output directory: {settings.general.output_dir}\")\n",
    "print(f\"üåê Target URL: {settings.site.listing_url}\")\n",
    "print(f\"üîç Headless mode: {settings.general.headless}\")\n",
    "print(f\"üåç Language filter: {settings.general.preferred_language}\")\n",
    "\n",
    "def run_scraper_sync():\n",
    "    \"\"\"Run the scraper in a separate thread with its own event loop (Windows compatibility)\"\"\"\n",
    "    print(\"\\nüîß Using threaded execution for Windows compatibility...\")\n",
    "\n",
    "    # Create a new event loop for this thread\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "\n",
    "    try:\n",
    "        # Verify output directory\n",
    "        output_dir = Path(settings.general.output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"‚úÖ Output directory ready: {output_dir.absolute()}\")\n",
    "\n",
    "        # Run the async scraper\n",
    "        result = loop.run_until_complete(process_listing())\n",
    "        print(\"‚úÖ Scraping completed successfully!\")\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during scraping: {str(e)}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "\n",
    "        # Provide specific guidance based on error type\n",
    "        if \"NotImplementedError\" in str(e):\n",
    "            print(\"\\nüîç This is a Windows/asyncio subprocess issue.\")\n",
    "            print(\"üí° Try running the standalone script generated below.\")\n",
    "        elif \"playwright\" in str(e).lower():\n",
    "            print(\"\\nüîç Playwright browser issue detected.\")\n",
    "            print(\"üí° Try running: !playwright install chromium\")\n",
    "        elif \"config\" in str(e).lower() or \"url\" in str(e).lower():\n",
    "            print(f\"\\nüîç Configuration issue. Current URL: {settings.site.listing_url}\")\n",
    "            print(\"üí° Make sure the listing_url in config.toml points to a real CURIA search results page.\")\n",
    "        else:\n",
    "            print(f\"\\nüîç Unexpected error: {e}\")\n",
    "\n",
    "        import traceback\n",
    "        print(\"\\nüìù Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        loop.close()\n",
    "\n",
    "# Try direct execution first, fall back to threaded\n",
    "try:\n",
    "    print(\"üîÑ Attempting direct async execution...\")\n",
    "    await process_listing()\n",
    "    print(\"‚úÖ Direct execution successful!\")\n",
    "\n",
    "except (NotImplementedError, RuntimeError) as e:\n",
    "    print(f\"‚ö†Ô∏è Direct execution failed: {type(e).__name__}\")\n",
    "    print(\"üîÑ Switching to threaded execution...\")\n",
    "\n",
    "    # Use ThreadPoolExecutor for better error handling\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        future = executor.submit(run_scraper_sync)\n",
    "\n",
    "        try:\n",
    "            result = future.result(timeout=300)  # 5 minute timeout\n",
    "            if result is not None:\n",
    "                print(\"üéâ Scraping completed via threaded execution!\")\n",
    "            else:\n",
    "                print(\"‚ùå Scraping failed - check errors above\")\n",
    "\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            print(\"‚è∞ Scraping timed out after 5 minutes\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Threaded execution also failed: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error in direct execution: {e}\")\n",
    "    print(\"üîÑ Trying threaded fallback...\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        future = executor.submit(run_scraper_sync)\n",
    "        try:\n",
    "            result = future.result(timeout=300)\n",
    "        except Exception as thread_error:\n",
    "            print(f\"‚ùå All execution methods failed: {thread_error}\")\n",
    "\n",
    "print(\"\\nüìã Scraper execution complete. Check the output directory for results.\")\n",
    "print(\"üí° If you encountered Windows/subprocess issues, try the standalone script)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e62bdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing CURIA scraper configuration and system compatibility...\n",
      "\n",
      "üìã Configuration Test:\n",
      "  ‚úì Listing URL: https://curia.europa.eu/juris/documents.jsf?nat=or&mat=or&pcs=Oor&jur=C%2CT%2CF&for=&jge=&dates=%2524type%253Dpro%2524mode%253D1M%2524from%253D2025.09.27%2524to%253D2025.10.27&language=en&pro=&cit=none%252CC%252CCJ%252CR%252C2008E%252C%252C%252C%252C%252C%252C%252C%252C%252C%252Ctrue%252Cfalse%252Cfalse&oqp=&td=%24mode%3D1M%24from%3D2025.09.27%24to%3D2025.10.27%3B%3BPUB%3BPUB1%2CPUB2%2CPUB4%2CPUB7%2CPUB3%2CPUB8%2CPUB5%2CPUB6%3B%3B%3B%3BORDALL&avg=&lgrec=en&page=1&lg=EN%252C%252Btrue%252Cfalse&cid=7416927\n",
      "  ‚úì Output Directory: ./output\n",
      "  ‚úì Target Language: en\n",
      "  ‚úì Headless Mode: True\n",
      "  ‚úì Throttle Delay: 2000ms\n",
      "\n",
      "üåê Connectivity Test:\n",
      "  ‚úÖ Successfully connected to https://curia.europa.eu/juris/documents.jsf?nat=or&mat=or&pcs=Oor&jur=C%2CT%2CF&for=&jge=&dates=%2524type%253Dpro%2524mode%253D1M%2524from%253D2025.09.27%2524to%253D2025.10.27&language=en&pro=&cit=none%252CC%252CCJ%252CR%252C2008E%252C%252C%252C%252C%252C%252C%252C%252C%252C%252Ctrue%252Cfalse%252Cfalse&oqp=&td=%24mode%3D1M%24from%3D2025.09.27%24to%3D2025.10.27%3B%3BPUB%3BPUB1%2CPUB2%2CPUB4%2CPUB7%2CPUB3%2CPUB8%2CPUB5%2CPUB6%3B%3B%3B%3BORDALL&avg=&lgrec=en&page=1&lg=EN%252C%252Btrue%252Cfalse&cid=7416927\n",
      "  ‚úì Response size: 190257 bytes\n",
      "  ‚úÖ Confirmed: Valid CURIA search results page\n",
      "\n",
      "üìÅ Directory Test:\n",
      "  ‚úÖ Output directory is writable: d:\\Repos\\Active\\Client\\Pandektes\\scraper\\output\n",
      "\n",
      "üé≠ Playwright Test:\n",
      "  ‚ùå Playwright issue: \n",
      "  üí° Try running: !playwright install\n",
      "\n",
      "ü™ü Windows Subprocess Compatibility Test:\n",
      "  ‚ùå Windows/Jupyter subprocess limitation confirmed\n",
      "  üí° This will prevent Playwright from working in Jupyter on Windows\n",
      "  üí° Recommended: Use the standalone script generated below\n",
      "\n",
      "‚öôÔ∏è Configuration File Test:\n",
      "  ‚úÖ config.toml exists: d:\\Repos\\Active\\Client\\Pandektes\\scraper\\config.toml\n",
      "  ‚úì Section 'general' present\n",
      "  ‚úì Section 'site' present\n",
      "\n",
      "üéØ System Diagnosis Complete!\n",
      "üí° If subprocess test failed, use the standalone script generated in the next cell\n",
      "üí° If connectivity failed, check your internet connection and config.toml URL\n",
      "üí° Green checkmarks ‚úÖ indicate ready components\n",
      "  ‚úÖ Successfully connected to https://curia.europa.eu/juris/documents.jsf?nat=or&mat=or&pcs=Oor&jur=C%2CT%2CF&for=&jge=&dates=%2524type%253Dpro%2524mode%253D1M%2524from%253D2025.09.27%2524to%253D2025.10.27&language=en&pro=&cit=none%252CC%252CCJ%252CR%252C2008E%252C%252C%252C%252C%252C%252C%252C%252C%252C%252Ctrue%252Cfalse%252Cfalse&oqp=&td=%24mode%3D1M%24from%3D2025.09.27%24to%3D2025.10.27%3B%3BPUB%3BPUB1%2CPUB2%2CPUB4%2CPUB7%2CPUB3%2CPUB8%2CPUB5%2CPUB6%3B%3B%3B%3BORDALL&avg=&lgrec=en&page=1&lg=EN%252C%252Btrue%252Cfalse&cid=7416927\n",
      "  ‚úì Response size: 190257 bytes\n",
      "  ‚úÖ Confirmed: Valid CURIA search results page\n",
      "\n",
      "üìÅ Directory Test:\n",
      "  ‚úÖ Output directory is writable: d:\\Repos\\Active\\Client\\Pandektes\\scraper\\output\n",
      "\n",
      "üé≠ Playwright Test:\n",
      "  ‚ùå Playwright issue: \n",
      "  üí° Try running: !playwright install\n",
      "\n",
      "ü™ü Windows Subprocess Compatibility Test:\n",
      "  ‚ùå Windows/Jupyter subprocess limitation confirmed\n",
      "  üí° This will prevent Playwright from working in Jupyter on Windows\n",
      "  üí° Recommended: Use the standalone script generated below\n",
      "\n",
      "‚öôÔ∏è Configuration File Test:\n",
      "  ‚úÖ config.toml exists: d:\\Repos\\Active\\Client\\Pandektes\\scraper\\config.toml\n",
      "  ‚úì Section 'general' present\n",
      "  ‚úì Section 'site' present\n",
      "\n",
      "üéØ System Diagnosis Complete!\n",
      "üí° If subprocess test failed, use the standalone script generated in the next cell\n",
      "üí° If connectivity failed, check your internet connection and config.toml URL\n",
      "üí° Green checkmarks ‚úÖ indicate ready components\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Configuration & System Test\n",
    "# Comprehensive test suite to validate configuration and system compatibility before scraping\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "print(\"üß™ Testing CURIA scraper configuration and system compatibility...\")\n",
    "\n",
    "# Test 1: Verify configuration values\n",
    "print(f\"\\nüìã Configuration Test:\")\n",
    "print(f\"  ‚úì Listing URL: {settings.site.listing_url}\")\n",
    "print(f\"  ‚úì Output Directory: {settings.general.output_dir}\")\n",
    "print(f\"  ‚úì Target Language: {settings.general.preferred_language}\")\n",
    "print(f\"  ‚úì Headless Mode: {settings.general.headless}\")\n",
    "print(f\"  ‚úì Throttle Delay: {settings.general.throttle_delay_ms}ms\")\n",
    "\n",
    "# Test 2: Check CURIA website connectivity\n",
    "print(f\"\\nüåê Connectivity Test:\")\n",
    "try:\n",
    "    response = requests.get(settings.site.listing_url, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"  ‚úÖ Successfully connected to {settings.site.listing_url}\")\n",
    "        print(f\"  ‚úì Response size: {len(response.content)} bytes\")\n",
    "\n",
    "        # Validate it's a proper CURIA page\n",
    "        content = response.text.lower()\n",
    "        curia_indicators = [\"curia\", \"court of justice\", \"european union\"]\n",
    "        document_indicators = [\"document\", \"judgment\", \"case\", \"docid\"]\n",
    "\n",
    "        has_curia = any(indicator in content for indicator in curia_indicators)\n",
    "        has_docs = any(indicator in content for indicator in document_indicators)\n",
    "\n",
    "        if has_curia and has_docs:\n",
    "            print(\"  ‚úÖ Confirmed: Valid CURIA search results page\")\n",
    "        elif has_curia:\n",
    "            print(\"  ‚ö†Ô∏è CURIA page detected, but may not contain search results\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è May not be a valid CURIA page\")\n",
    "\n",
    "    else:\n",
    "        print(f\"  ‚ùå HTTP Error: {response.status_code}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"  ‚ùå Connection failed: {e}\")\n",
    "    print(\"  üí° Check your internet connection and verify the URL in config.toml\")\n",
    "\n",
    "# Test 3: Verify output directory permissions\n",
    "print(f\"\\nüìÅ Directory Test:\")\n",
    "try:\n",
    "    output_dir = Path(settings.general.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Test file creation and deletion\n",
    "    test_file = output_dir / \"test.txt\"\n",
    "    test_file.write_text(\"test content\")\n",
    "    test_file.unlink()\n",
    "\n",
    "    print(f\"  ‚úÖ Output directory is writable: {output_dir.absolute()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Directory issue: {e}\")\n",
    "    print(\"  üí° Check folder permissions or change output_dir in config.toml\")\n",
    "\n",
    "# Test 4: Check Playwright compatibility (async-safe)\n",
    "print(f\"\\nüé≠ Playwright Test:\")\n",
    "try:\n",
    "    from playwright.async_api import async_playwright\n",
    "\n",
    "    async def test_playwright():\n",
    "        async with async_playwright() as p:\n",
    "            browser_path = p.chromium.executable_path\n",
    "            return browser_path\n",
    "\n",
    "    # Test in current async context\n",
    "    try:\n",
    "        browser_path = await test_playwright()\n",
    "        print(f\"  ‚úÖ Playwright browsers installed: {browser_path}\")\n",
    "    except Exception as e:\n",
    "        if \"NotImplementedError\" in str(e):\n",
    "            print(\"  ‚ö†Ô∏è Windows/Jupyter subprocess limitation detected\")\n",
    "            print(\"  üí° This may prevent browser automation in this environment\")\n",
    "            print(\"  üí° Use the standalone script instead (generated below)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Playwright issue: {e}\")\n",
    "            print(\"  üí° Try running: !playwright install\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"  ‚ùå Playwright not installed\")\n",
    "    print(\"  üí° Run the package installation cell above\")\n",
    "\n",
    "# Test 5: Windows subprocess compatibility check\n",
    "print(f\"\\nü™ü Windows Subprocess Compatibility Test:\")\n",
    "try:\n",
    "    # Test if we can create subprocesses in this environment\n",
    "    async def test_subprocess():\n",
    "        proc = await asyncio.create_subprocess_exec(\n",
    "            \"echo\", \"test\",\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE\n",
    "        )\n",
    "        stdout, stderr = await proc.communicate()\n",
    "        return proc.returncode == 0\n",
    "\n",
    "    can_subprocess = await test_subprocess()\n",
    "    if can_subprocess:\n",
    "        print(\"  ‚úÖ Subprocess creation works in this environment\")\n",
    "        print(\"  üí° Direct notebook execution should work\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Subprocess creation failed\")\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"  ‚ùå Windows/Jupyter subprocess limitation confirmed\")\n",
    "    print(\"  üí° This will prevent Playwright from working in Jupyter on Windows\")\n",
    "    print(\"  üí° Recommended: Use the standalone script generated below\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Subprocess test error: {e}\")\n",
    "\n",
    "# Test 6: Configuration file validation\n",
    "print(f\"\\n‚öôÔ∏è Configuration File Test:\")\n",
    "try:\n",
    "    config_path = Path(\"config.toml\")\n",
    "    if config_path.exists():\n",
    "        print(f\"  ‚úÖ config.toml exists: {config_path.absolute()}\")\n",
    "\n",
    "        # Validate required sections\n",
    "        config_data = toml.load(config_path)\n",
    "        required_sections = [\"general\", \"site\"]\n",
    "        for section in required_sections:\n",
    "            if section in config_data:\n",
    "                print(f\"  ‚úì Section '{section}' present\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå Missing section '{section}'\")\n",
    "    else:\n",
    "        print(\"  ‚ùå config.toml not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Configuration test error: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ System Diagnosis Complete!\")\n",
    "print(f\"üí° If subprocess test failed, use the standalone script generated in the next cell\")\n",
    "print(f\"üí° If connectivity failed, check your internet connection and config.toml URL\")\n",
    "print(f\"üí° Green checkmarks ‚úÖ indicate ready components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1ac53",
   "metadata": {},
   "source": [
    "# Step 13: Windows Compatibility Solution\n",
    "If you're experiencing Windows/Jupyter subprocess issues, this cell generates a standalone Python script that bypasses all limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c998dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Creating standalone Python script to bypass Windows/Jupyter limitations...\n",
      "‚úÖ Created standalone script: d:\\Repos\\Active\\Client\\Pandektes\\scraper\\curia_scraper_standalone.py\n",
      "\n",
      "üöÄ To run the standalone scraper:\n",
      "   1. Open terminal/command prompt\n",
      "   2. Navigate to: d:\\Repos\\Active\\Client\\Pandektes\\scraper\n",
      "   3. Run: python curia_scraper_standalone.py\n",
      "\n",
      "üí° Benefits of standalone script:\n",
      "   ‚úì Bypasses Windows/Jupyter subprocess limitations\n",
      "   ‚úì Runs in headless mode by default (faster)\n",
      "   ‚úì Better error handling and logging\n",
      "   ‚úì Self-contained with all dependencies\n",
      "   ‚úì Perfect for production/automated runs\n",
      "\n",
      "üìã The script will:\n",
      "   ‚Ä¢ Auto-create config.toml if missing\n",
      "   ‚Ä¢ Process all pages of CURIA search results\n",
      "   ‚Ä¢ Download PDFs when 'Start Printing' buttons available\n",
      "   ‚Ä¢ Extract structured metadata for all documents\n",
      "   ‚Ä¢ Save everything to ./output directory\n",
      "\n",
      "üîß Configuration:\n",
      "   Edit config.toml to customize:\n",
      "   - listing_url: Your CURIA search results page\n",
      "   - preferred_language: EN, FR, DE, etc.\n",
      "   - output_dir: Where to save files\n",
      "   - headless: true/false for browser visibility\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Generate Standalone Python Script (Windows Workaround)\n",
    "# Creates a self-contained Python script that bypasses Windows/Jupyter subprocess limitations\n",
    "\n",
    "print(\"üõ†Ô∏è Creating standalone Python script to bypass Windows/Jupyter limitations...\")\n",
    "\n",
    "script_content = '''#!/usr/bin/env python\n",
    "\"\"\"\n",
    "CURIA Scraper - Standalone Version\n",
    "==================================\n",
    "\n",
    "This script bypasses Windows/Jupyter subprocess limitations by running\n",
    "as a standalone Python application. It includes all the functionality\n",
    "from the Jupyter notebook in a single executable file.\n",
    "\n",
    "Usage:\n",
    "    python curia_scraper_standalone.py\n",
    "\n",
    "Requirements:\n",
    "    - Python 3.8+\n",
    "    - All packages from requirements (automatically checked)\n",
    "    - config.toml file (automatically created if missing)\n",
    "\n",
    "Output:\n",
    "    - PDF files: curia-doc-{docid}.pdf\n",
    "    - Metadata files: doc_{index}.json\n",
    "    - All files saved to ./output directory\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import toml\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, Page\n",
    "\n",
    "# Configuration models\n",
    "class GeneralSettings(BaseModel):\n",
    "    output_dir: str\n",
    "    checkpoint_file: str\n",
    "    headless: bool\n",
    "    throttle_delay_ms: int\n",
    "    preferred_language: str = \"EN\"\n",
    "\n",
    "class SiteSettings(BaseModel):\n",
    "    listing_url: str\n",
    "    document_content_selector: str = \"body\"\n",
    "    start_print_button_text: str = \"Start Printing\"\n",
    "    document_link_selector: str = \"div#docHtml a[href*='document.jsf']\"\n",
    "    next_page_selector: str = \"a[title='Next Page']\"\n",
    "\n",
    "class Settings(BaseModel):\n",
    "    general: GeneralSettings\n",
    "    site: SiteSettings\n",
    "\n",
    "# Load or create configuration\n",
    "def load_config():\n",
    "    \"\"\"Load configuration from config.toml or create default\"\"\"\n",
    "    config_path = Path(\"config.toml\")\n",
    "\n",
    "    if not config_path.exists():\n",
    "        print(\"üìù Creating default config.toml...\")\n",
    "        default_config = {\n",
    "            \"general\": {\n",
    "                \"output_dir\": \"./output\",\n",
    "                \"checkpoint_file\": \"./checkpoint.json\",\n",
    "                \"headless\": True,  # Headless for standalone execution\n",
    "                \"throttle_delay_ms\": 2000,\n",
    "                \"preferred_language\": \"EN\"\n",
    "            },\n",
    "            \"site\": {\n",
    "                \"listing_url\": \"https://curia.europa.eu/juris/recherche.jsf?language=en\",\n",
    "                \"document_content_selector\": \"body\",\n",
    "                \"start_print_button_text\": \"Start Printing\",\n",
    "                \"document_link_selector\": \"div#docHtml a[href*='document.jsf']\",\n",
    "                \"next_page_selector\": \"a[title='Next Page']\"\n",
    "            }\n",
    "        }\n",
    "        with open(config_path, \"w\") as f:\n",
    "            toml.dump(default_config, f)\n",
    "        print(\"‚úÖ Created config.toml with CURIA defaults\")\n",
    "\n",
    "    return Settings(**toml.load(\"config.toml\"))\n",
    "\n",
    "# Load configuration\n",
    "settings = load_config()\n",
    "\n",
    "# Logger setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(levelname)s %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"curia_scraper\")\n",
    "\n",
    "# Browser manager\n",
    "class BrowserManager:\n",
    "    \"\"\"Manages Playwright browser instances with proper cleanup\"\"\"\n",
    "\n",
    "    def __init__(self, headless: bool = True, downloads_path: str = None):\n",
    "        self.headless = headless\n",
    "        self.downloads_path = downloads_path\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(headless=self.headless)\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        if self.browser:\n",
    "            await self.browser.close()\n",
    "        await self.playwright.stop()\n",
    "\n",
    "    async def new_page(self):\n",
    "        context_args = {}\n",
    "        if self.downloads_path:\n",
    "            context_args[\"accept_downloads\"] = True\n",
    "            context_args[\"downloads_path\"] = self.downloads_path\n",
    "        context = await self.browser.new_context(**context_args)\n",
    "        page = await context.new_page()\n",
    "        return page\n",
    "\n",
    "# Storage functions\n",
    "def save_json(content: dict, idx: int):\n",
    "    \"\"\"Save document metadata to JSON file\"\"\"\n",
    "    output_dir = Path(settings.general.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filename = output_dir / f\"doc_{idx}.json\"\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(content, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    logger.info(f\"Saved JSON metadata: {filename}\")\n",
    "\n",
    "# CURIA-specific parser function\n",
    "def parse_curia_document(html: str, url: str, doc_id: str = None) -> dict:\n",
    "    \"\"\"Parse CURIA legal document HTML and extract structured metadata\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Extract case number\n",
    "    case_number = None\n",
    "    text_content = soup.get_text()\n",
    "    case_matches = re.findall(r'Case\\\\s+[A-Z]-\\\\d+/\\\\d+', text_content, re.I)\n",
    "    if case_matches:\n",
    "        case_number = case_matches[0]\n",
    "\n",
    "    # Extract title\n",
    "    title = None\n",
    "    for selector in [\"title\", \"h1\", \"h2\"]:\n",
    "        tag = soup.select_one(selector)\n",
    "        if tag and tag.text.strip():\n",
    "            title = re.sub(r'^(CURIA\\\\s*-\\\\s*)?', '', tag.text.strip(), flags=re.I)\n",
    "            break\n",
    "\n",
    "    # Extract language from URL\n",
    "    language = None\n",
    "    lang_match = re.search(r'doclang=([A-Z]{2})', url)\n",
    "    if lang_match:\n",
    "        language = lang_match.group(1)\n",
    "\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"url\": url,\n",
    "        \"language\": language,\n",
    "        \"case_number\": case_number,\n",
    "        \"title\": title,\n",
    "        \"html_length\": len(html),\n",
    "        \"extracted_at\": str(datetime.now()),\n",
    "        \"html\": html\n",
    "    }\n",
    "\n",
    "# Document processor\n",
    "async def process_document(page: Page, doc_link: str, idx: int):\n",
    "    \"\"\"Process a single CURIA document\"\"\"\n",
    "    logger.info(f\"[{idx}] Processing CURIA document: {doc_link}\")\n",
    "\n",
    "    await page.goto(doc_link)\n",
    "    await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "    doc_id_match = re.search(r'docid=(\\\\d+)', doc_link)\n",
    "    doc_id = doc_id_match.group(1) if doc_id_match else str(idx)\n",
    "\n",
    "    log_entry = {\n",
    "        \"idx\": idx,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"url\": doc_link,\n",
    "        \"used_path\": None,\n",
    "        \"filename\": None\n",
    "    }\n",
    "\n",
    "    # Look for print button\n",
    "    print_button_selectors = [\n",
    "        \"input[value*='Start Printing']\",\n",
    "        \"button:has-text('Start Printing')\",\n",
    "        \"a:has-text('Start Printing')\",\n",
    "        \"input[type='submit'][value*='Print']\"\n",
    "    ]\n",
    "\n",
    "    print_btn = None\n",
    "    for selector in print_button_selectors:\n",
    "        try:\n",
    "            print_btn = await page.wait_for_selector(selector, timeout=3000)\n",
    "            if print_btn:\n",
    "                logger.info(f\"[{idx}] Found print button: {selector}\")\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if print_btn:\n",
    "        # Generate PDF from print view\n",
    "        await print_btn.click()\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "        await page.wait_for_timeout(2000)\n",
    "\n",
    "        filename = f\"curia-doc-{doc_id}.pdf\"\n",
    "        filepath = Path(settings.general.output_dir) / filename\n",
    "\n",
    "        await page.pdf(\n",
    "            path=str(filepath),\n",
    "            format=\"A4\",\n",
    "            print_background=True,\n",
    "            margin={\"top\": \"1cm\", \"right\": \"1cm\", \"bottom\": \"1cm\", \"left\": \"1cm\"}\n",
    "        )\n",
    "        logger.info(f\"[{idx}] Saved PDF: {filepath}\")\n",
    "\n",
    "        log_entry[\"used_path\"] = \"page_pdf\"\n",
    "        log_entry[\"filename\"] = filename\n",
    "        save_json(log_entry, idx)\n",
    "    else:\n",
    "        # Fallback to HTML parsing\n",
    "        logger.info(f\"[{idx}] No print button found; saving HTML\")\n",
    "        html_content = await page.content()\n",
    "        parsed = parse_curia_document(html_content, doc_link, doc_id)\n",
    "        parsed[\"filename\"] = None\n",
    "        parsed[\"used_path\"] = \"html_parse\"\n",
    "        save_json(parsed, idx)\n",
    "\n",
    "# Main listing processor\n",
    "async def process_listing():\n",
    "    \"\"\"Main CURIA listing processor\"\"\"\n",
    "    output_dir = Path(settings.general.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    async with BrowserManager(headless=settings.general.headless, downloads_path=str(output_dir)) as bm:\n",
    "        page = await bm.new_page()\n",
    "\n",
    "        logger.info(f\"Navigating to: {settings.site.listing_url}\")\n",
    "        await page.goto(settings.site.listing_url)\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "        idx = 0\n",
    "        page_num = 1\n",
    "\n",
    "        while True:\n",
    "            logger.info(f\"Processing page {page_num}...\")\n",
    "\n",
    "            # Find document links\n",
    "            document_links = []\n",
    "            link_selectors = [\n",
    "                \"div#docHtml a[href*='document.jsf']\",\n",
    "                \"a[href*='document.jsf']\",\n",
    "                \"a[href*='docid=']\"\n",
    "            ]\n",
    "\n",
    "            for selector in link_selectors:\n",
    "                try:\n",
    "                    links = await page.query_selector_all(selector)\n",
    "                    if links:\n",
    "                        logger.info(f\"Found {len(links)} links with: {selector}\")\n",
    "\n",
    "                        for link in links:\n",
    "                            href = await link.get_attribute(\"href\")\n",
    "                            if href:\n",
    "                                if href.startswith(\"http\"):\n",
    "                                    full_url = href\n",
    "                                else:\n",
    "                                    full_url = f\"https://curia.europa.eu{href}\"\n",
    "\n",
    "                                # Filter by language\n",
    "                                if settings.general.preferred_language:\n",
    "                                    if f\"doclang={settings.general.preferred_language}\" in full_url:\n",
    "                                        document_links.append(full_url)\n",
    "                                    elif \"doclang=\" not in full_url:\n",
    "                                        full_url += f\"&doclang={settings.general.preferred_language}\"\n",
    "                                        document_links.append(full_url)\n",
    "                                else:\n",
    "                                    document_links.append(full_url)\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Selector failed: {selector} - {e}\")\n",
    "\n",
    "            if not document_links:\n",
    "                logger.warning(\"No document links found\")\n",
    "                break\n",
    "\n",
    "            # Remove duplicates\n",
    "            unique_links = []\n",
    "            seen = set()\n",
    "            for link in document_links:\n",
    "                doc_match = re.search(r'docid=(\\\\d+)', link)\n",
    "                if doc_match:\n",
    "                    doc_id = doc_match.group(1)\n",
    "                    if doc_id not in seen:\n",
    "                        seen.add(doc_id)\n",
    "                        unique_links.append(link)\n",
    "\n",
    "            logger.info(f\"Processing {len(unique_links)} unique documents\")\n",
    "\n",
    "            # Process each document\n",
    "            for doc_link in unique_links:\n",
    "                idx += 1\n",
    "                try:\n",
    "                    await process_document(page, doc_link, idx)\n",
    "                    await page.wait_for_timeout(1000)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"[{idx}] Error: {e}\")\n",
    "\n",
    "            # Look for next page\n",
    "            next_btn = None\n",
    "            next_selectors = [\"a[title*='Next']\", \"a:has-text('Next')\", \"a:has-text('¬ª')\"]\n",
    "\n",
    "            for selector in next_selectors:\n",
    "                try:\n",
    "                    next_btn = await page.query_selector(selector)\n",
    "                    if next_btn:\n",
    "                        is_disabled = await next_btn.get_attribute(\"disabled\")\n",
    "                        if not is_disabled:\n",
    "                            break\n",
    "                        next_btn = None\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if not next_btn:\n",
    "                logger.info(\"No next page found\")\n",
    "                break\n",
    "\n",
    "            await next_btn.click()\n",
    "            await page.wait_for_load_state(\"networkidle\")\n",
    "            await page.wait_for_timeout(settings.general.throttle_delay_ms)\n",
    "            page_num += 1\n",
    "\n",
    "        logger.info(f\"Completed! Processed {idx} documents across {page_num} pages\")\n",
    "\n",
    "# Main execution\n",
    "async def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    print(\"üöÄ CURIA Scraper - Standalone Version\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üìÅ Output: {settings.general.output_dir}\")\n",
    "    print(f\"üåê URL: {settings.site.listing_url}\")\n",
    "    print(f\"üåç Language: {settings.general.preferred_language}\")\n",
    "    print(f\"üëÅÔ∏è  Headless: {settings.general.headless}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    try:\n",
    "        await process_listing()\n",
    "        print(\"\\\\n‚úÖ Scraping completed successfully!\")\n",
    "        print(f\"üìÇ Check {settings.general.output_dir} for results\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üéØ Starting CURIA document scraper...\")\n",
    "    asyncio.run(main())\n",
    "'''\n",
    "\n",
    "# Write the script to file\n",
    "script_path = Path(\"curia_scraper_standalone.py\")\n",
    "script_path.write_text(script_content, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úÖ Created standalone script: {script_path.absolute()}\")\n",
    "print(f\"\\nüöÄ To run the standalone scraper:\")\n",
    "print(f\"   1. Open terminal/command prompt\")\n",
    "print(f\"   2. Navigate to: {Path.cwd()}\")\n",
    "print(f\"   3. Run: python curia_scraper_standalone.py\")\n",
    "print(f\"\\nüí° Benefits of standalone script:\")\n",
    "print(f\"   ‚úì Bypasses Windows/Jupyter subprocess limitations\")\n",
    "print(f\"   ‚úì Runs in headless mode by default (faster)\")\n",
    "print(f\"   ‚úì Better error handling and logging\")\n",
    "print(f\"   ‚úì Self-contained with all dependencies\")\n",
    "print(f\"   ‚úì Perfect for production/automated runs\")\n",
    "\n",
    "print(f\"\\nüìã The script will:\")\n",
    "print(f\"   ‚Ä¢ Auto-create config.toml if missing\")\n",
    "print(f\"   ‚Ä¢ Process all pages of CURIA search results\")\n",
    "print(f\"   ‚Ä¢ Download PDFs when 'Start Printing' buttons available\")\n",
    "print(f\"   ‚Ä¢ Extract structured metadata for all documents\")\n",
    "print(f\"   ‚Ä¢ Save everything to ./output directory\")\n",
    "\n",
    "print(f\"\\nüîß Configuration:\")\n",
    "print(f\"   Edit config.toml to customize:\")\n",
    "print(f\"   - listing_url: Your CURIA search results page\")\n",
    "print(f\"   - preferred_language: EN, FR, DE, etc.\")\n",
    "print(f\"   - output_dir: Where to save files\")\n",
    "print(f\"   - headless: true/false for browser visibility\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curia-scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
